[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"","tags":null,"title":"Jiaming Shen","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536447600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536447600,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00+01:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906545600,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":[],"content":"  使用Horseshoe 先验的Bayes回归及代码解析 Horseshoe prior是一种稀疏bayes监督学习的方法。通过对模型参数的先验分布中加入稀疏特征，从而得到稀疏的估计。\nhorseshoe prior属于multivariate scale mixtures of normals的分布族。所以和其他常用的稀疏bayes学习方法，Laplacian prior, (Lasso), Student-t prior，非常类似。\n在回归问题下的模型假设： \\[ y=X\\beta+\\epsilon, \\epsilon\\sim N(0,\\sigma^2)\\\\ \\beta_j \\sim N(0,\\sigma^2 \\lambda_j^2 \\tau^2)\\\\ \\lambda_j \\sim Half-Cauchy(0,1) \\] 可以额外加上以下超参数模型 \\[ \\tau \\sim Half-Cauchy (0,1)\\\\ \\sigma^2 \\sim 1/\\sigma^2 \\]\n其中\\(\\lambda_j\\) 叫做local shrinkage parameter，局部压缩参数，对于不同的\\(\\theta_j\\)可以有不同的压缩系数，\\(\\tau\\) 叫做global shrinkage parameter\n叫horseshoe，马蹄的原因大概是shrinkage weights的图像呈现马蹄形。\n@carlson 2010 给出了几个horseshoe estimator的相关性质以及和传统其他压缩先验方法的比较。\n这篇文章给出了一个用于判断各种压缩先验性质的方法: \\[ E\\left(\\theta_{i} \\mid y\\right)=\\int_{0}^{1}\\left(1-\\kappa_{i}\\right) y_{i} p\\left(\\kappa_{i} \\mid y\\right) \\mathrm{d} \\kappa_{i}=\\left\\{1-E\\left(\\kappa_{i} \\mid y\\right)\\right\\} y_{i} \\] 在这种情况下, \\(E(\\kappa_i|y)\\) 就是压缩率。\n而horseshoe prior在这个指标下就相对很优秀：对于很小的参数，压缩效果明显，对于相对比较大的参数，几乎不压缩。\nR包horseshoe提供了horseshoe estimator的计算函数，而本文主要目的就是对horseshoe.R中的算法进行整合和解析。\nhorseshoe = function(y,X, method.tau = c(\u0026quot;fixed\u0026quot;, \u0026quot;truncatedCauchy\u0026quot;,\u0026quot;halfCauchy\u0026quot;), tau = 1, method.sigma = c(\u0026quot;fixed\u0026quot;, \u0026quot;Jeffreys\u0026quot;), Sigma2 = 1, burn = 1000, nmc = 5000, thin = 1, alpha = 0.05) 函数的定义，对于 \\(\\tau\\) 可以选择常数，截尾柯西分布和半柯西分布，默认的tau是1，同样 \\(\\sigma\\) 也可以选择固定值和Jeffreys prior (\\(\\sim 1/\\sigma^2\\)) 。\nif(p\u0026gt;n) algo=1 else algo=2 在函数介绍中就有，当p\u0026gt;n时，使用的是Bhattacharya et.al. (2016)的算法，当n\u0026gt;p时，使用的是 Rue (2001)的算法对\\(\\beta\\) 的full conditional posterior distribution进行抽样。\n因为是Normal-Normal case，conditional on \\(\\sigma,\\tau,\\lambda\\) ,\\(\\beta\\) 是Normal prior加Normal likelihood，所以对于Bayes回归问题，beta的全条件后验分布基本上可以写成: \\[ \\beta \\mid y, \\lambda, \\tau, \\sigma \\sim N\\left(A^{-1} X^{\\mathrm{T}} y, \\sigma^{2} A^{-1}\\right), \\quad A=\\left(X^{\\mathrm{T}} X+\\Lambda_{*}^{-1}\\right), \\quad \\Lambda_{*}=\\tau^{2} \\operatorname{diag}\\left(\\lambda_{1}^{2}, \\ldots, \\lambda_{p}^{2}\\right) \\] 这种形式，所以就是用Gibbs算法对\\(\\beta\\) 抽样。\n现在介绍在p\u0026gt;n情况下的算法。\n来源于Bhattacharya. et.al. (2016)，\n假设我们要从\\(N(\\mu,\\Sigma)\\)中抽样，有 \\[ \\Sigma=\\left(\\Phi^{\\mathrm{T}} \\Phi+D^{-1}\\right)^{-1}, \\quad \\mu=\\Sigma \\Phi^{\\mathrm{T}} \\alpha \\] 算法如下\n抽 \\(u\\sim N(0,D)\\) , \\(\\delta\\sim N(0,I_n)\\) 令 \\(v=\\Phi u+\\delta\\) 解线性方程\\((\\Phi D \\Phi^T+I_n)w=(\\alpha-v)\\) 得到w 令\\(\\theta=u+D\\Phi^T w\\)  算法的推导过程是：\n我们目标是从\\(N(\\mu,\\Sigma)\\) 中进行抽样，由于 \\[ \\Sigma=(\\Phi^T\\Phi+D^{-1})^{-1}=D-D\\Phi(\\Phi D\\Phi^T+I_n^{-1})^{-1}\\Phi D \\] 那么第一块D，则可以通过抽\\(u\\sim N(0,D)\\) 直接得到，也就是算法第4步中的\\(u\\). 问题就是怎么得到第二块。\n第二块等于 \\[ \\begin{align*} \u0026amp;D\\Phi(\\Phi D\\Phi^T+I_n^{-1})^{-1}\\Phi D\\\\ =\u0026amp;D\\Phi^T(\\Phi D\\Phi^T+I_n)^{-1}(\\Phi D \\Phi ^T+I_n)(\\Phi D\\Phi^T+I_n)^{-1}\\Phi D \\end{align*} \\] 因为 \\[ \\begin{align*} \\mu\u0026amp;=\\Sigma\\Phi^T\\alpha=(\\Phi^T\\Phi+D^{-1})^{-1}\\Phi^T\\alpha=(D\\Phi^T-D\\Phi^T(\\Phi D\\Phi^T+I_n)^{-1}\\Phi D\\Phi^T)\\alpha\\\\ \u0026amp;=D\\Phi^T (\\Phi D\\Phi^T+I_n)^{-1}\\alpha \\end{align*} \\] 所以相当于只需要对\\(N(0,(\\Phi D\\Phi^T+I_n))\\) 加上\\(\\alpha\\)再做线性变换 \\(D\\Phi^T\\cdot (\\Phi D\\Phi^{T}+I_n)^{-1}\\) 就能得到第二块的分布，也就对应算法过程3和4。\n而为了得到\\(N(0,(\\Phi D\\Phi^T+I_n))\\)，则只需要对\\(u\\sim N(0,D)\\) 乘以\\(\\Phi\\) 加\\(\\delta \\sim N(0,I_n)\\)， 对应算法过程2。\n这样的算法过程就对应代码：\n lambda_star=tau*lambda U=as.numeric(lambda_star^2)*t(X) ## step 1 ## u=stats::rnorm(l2,l0,lambda_star) v=X%*%u + stats::rnorm(n) ## step 2 ## v_star=solve((X%*%U+I_n),((y/sqrt(sigma_sq))-v)) Beta=sqrt(sigma_sq)*(u+U%*%v_star) 当p\u0026lt;n,也就是algo==2时则是用经典的Cholesky分解进行抽样，故不再赘述。\n下一块则是抽取\\(\\lambda_j\\) 的过程，这块算法使用了 Polson. et. al. (2014)的supplement material中，使用Damien et.al. (1999)算法构造的Slice sampler。\n首先，我们可以写出\\(\\lambda_j\\) 的后验分布： \\[ p(\\lambda_j|y,\\tau,\\sigma)\\propto \\frac{2}{\\pi(1+\\lambda_j^2)}\\cdot \\frac{1}{\\lambda_j}\\cdot exp(-\\frac{1}{\\lambda_j^2}\\frac{\\beta_j^2}{2\\tau^2\\sigma^2} ) \\] 做变量代换: \\(\\eta_j=1/\\lambda_j^2\\), \\(\\mu_j=\\beta_j/(\\tau\\sigma)\\). 那么\\(\\lambda_j\\) 的full-conditional distribution转变为\\(\\eta_j\\)的 full conditional distribution,由变量代换公式: \\[ p(\\eta_j|y,\\tau,\\sigma)\\propto \\frac{1}{\\eta_j+1}\\cdot exp(-\\frac{\\mu_j^2}{2}\\eta_j) \\] 然后通过Damien et. al. (1999)的Slice sampler:\n如果要生成密度函数为\\(f\\) 的随机数，\\(f\\) 可以写成如下形式: \\[ f(x) \\propto \\pi(x) \\prod_{i=1}^{N} l_{i}(x) \\] 其中\\(\\pi(x)\\) 是已知形式的密度函数，\\(l_i\\) 是非负可逆函数，也就是如果有 \\(l_i(x)\u0026gt;u\\)，那么可以反推出\\(A^i_u=\\{x:l_i(x)\u0026gt;u \\}\\)。那么就可以通过添加辅助变量的方法生成服从\\(f\\) 的随机数。\n令辅助变量 \\(u_i|x \\sim unif (0,l_i(x))\\), 那么\\(U\\) 和\\(x\\) 的联合分布就是 \\[ f\\left(x, u_{1}, \\ldots, u_{N}\\right) \\propto \\pi(x) \\prod_{i=1}^{N} I\\left\\{u_{i}\u0026lt;l_{i}(x)\\right\\} \\] 这样，关于X的边际分布就是\\(f(x)\\)。我们构造Gibbs sampler，其中\\(u_i\\) 的full conditional distribution就是够早的uniform, 而X的full conditional distribution，则是分布\\(\\pi\\), 但是局限在区域\\(A_{u}=\\left\\{x: l_{i}(x)\u0026gt;u_{i}, i=1, \\ldots, N\\right\\}\\).\n因为\\(\\pi\\)是已知好抽的分布，\\(l_i\\)是可逆函数，所以就把问题变成求区域和从好抽的分布中抽的问题。\n应用在如上例子中，就是 \\(l(\\eta_j)=\\frac{1}{\\eta_j+1}\\) , \\(\\pi(\\eta_j)=exp(-\\frac{\\mu_j^2}{2}\\eta_j)\\) 是指数分布。\n那算法：\n从\\((0,l(\\eta_j))\\)中抽\\(u\\) .\n 求区域 \\(A_u = \\{\\eta_j,l(\\eta_j)\u0026gt;u\\}\\), 也就是\\((0,\\frac{1}{u}-1)\\)\n 在限制于\\((0,\\frac{1}{u}-1)\\) 的exp分布上抽\\(\\eta_j\\)。\n  这样就能得到\\(\\eta_j\\)。然后再逆变化回\\(\\lambda_j\\) 就能得到\\(\\lambda_j\\) 的样本，又因为\\(\\lambda_j\\)之间是独立的，所以这一步能向量化操作同时对所有的p个\\(\\lambda_j\\)抽样。\n其中第三步的算法实现是：\n由于局限在了\\((0,\\frac{1}{u}-1)\\) 上，而指数分布是单调的。就可以用一般的分布抽样的方法，从定义在\\((0, F(\\frac{1}{u}-1))\\)的uniform抽，F是累积分布函数, 在这里就是 \\(1-exp(-\\mu_j\\cdot (\\frac{1}{u}-1))\\), 然后再逆回去，得到目标随机数。\n算法：\n从\\((0,1-exp(-\\mu_j\\cdot (\\frac{1}{u}-1)))\\) 中抽 \\(a\\) \\(\\eta_j= -log(1-a)/\\mu_j\\)  对应代码：\n## update lambda_j\u0026#39;s in a block using slice sampling ## eta = 1/(lambda^2) upsi = stats::runif(p,0,1/(1+eta)) tempps = Beta^2/(2*sigma_sq*tau^2) ub = (1-upsi)/upsi # now sample eta from exp(tempv) truncated between 0 \u0026amp; upsi/(1-upsi) Fub = 1 - exp(-tempps*ub) # exp cdf at ub Fub[Fub \u0026lt; (1e-4)] = 1e-4; # for numerical stability up = stats::runif(p,0,Fub) eta = -log(1-up)/tempps lambda = 1/sqrt(eta); 剩下的更新\\(\\tau\\) 的部分类似。但是由于\\(\\tau\\)是全局的参数，所以单个式子的形式就变成了叠乘的形式，所以就从exp分布变味了gamma分布。\nThe density of \\(\\tau\\)\nGamma distribution \\(f(x)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}\\)\n ","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"04f3f2c4cb032488068c85ce2a360048","permalink":"/post/horseshoe-bayes/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/post/horseshoe-bayes/","section":"post","summary":"使用Horseshoe 先验的Bayes回归及代码解析 Horseshoe prior是一种稀疏bayes监督学习的方法。通过对模型参数的先验分布中加入稀疏特征，从而得到稀疏的估计。\nhorseshoe prior属于multivariate scale mixtures of normals的分布族。所以和其他常用的稀疏bayes学习方法，Laplacian prior, (Lasso), Student-t prior，非常类似。\n在回归问题下的模型假设： \\[ y=X\\beta+\\epsilon, \\epsilon\\sim N(0,\\sigma^2)\\\\ \\beta_j \\sim N(0,\\sigma^2 \\lambda_j^2 \\tau^2)\\\\ \\lambda_j \\sim Half-Cauchy(0,1) \\] 可以额外加上以下超参数模型 \\[ \\tau \\sim Half-Cauchy (0,1)\\\\ \\sigma^2 \\sim 1/\\sigma^2 \\]\n其中\\(\\lambda_j\\) 叫做local shrinkage parameter，局部压缩参数，对于不同的\\(\\theta_j\\)可以有不同的压缩系数，\\(\\tau\\) 叫做global shrinkage parameter\n叫horseshoe，马蹄的原因大概是shrinkage weights的图像呈现马蹄形。\n@carlson 2010 给出了几个horseshoe estimator的相关性质以及和传统其他压缩先验方法的比较。\n这篇文章给出了一个用于判断各种压缩先验性质的方法: \\[ E\\left(\\theta_{i} \\mid y\\right)=\\int_{0}^{1}\\left(1-\\kappa_{i}\\right) y_{i} p\\left(\\kappa_{i} \\mid y\\right) \\mathrm{d} \\kappa_{i}=\\left\\{1-E\\left(\\kappa_{i} \\mid y\\right)\\right\\} y_{i} \\] 在这种情况下, \\(E(\\kappa_i|y)\\) 就是压缩率。","tags":[],"title":"使用Horseshoe 先验的Bayes回归及代码解析","type":"post"},{"authors":null,"categories":["随笔"],"content":"作为谢大的迷弟，如何想静静 读后颇有感触，随便写点。也算是另一种程度上的静静。虽然高质量产出很缺乏也很难，但是低质量产出或多或少还是可以做。\n先说第一部分，昨天参加了华为在曼大的宣讲会，听各方面的研究和各个部门代表讲话。感觉嗯大家都很充实的改变世界，除了我自己在混日子（笑）。这种割裂感来源很多，其中一点大概是做项目的感觉把。我曾经也觉得那些天才好厉害天经地纬啥的智商高果然不一样。但是慢慢的就觉得积累，环境，等等的都会有很大的影响作用。人脑解决问题的方式应该还是简化简化积累简化。也就是说处理的每一个小问题都应该是简单的，只不过这个简单对积累不同的人不一样。能把微积分常见的定理+trick倒背如流做吉米多维奇时的难度肯定就比只是去上课听了一点还记不住的人简单。但是处理的时候还是以处理简单问题为主。而同样类比到博士，成熟的试验手段，代码积累，以及人的问题都很重要。因为很多复杂的问题都是很多人积累之下，而熟悉自己课题组的积累比从论文中无众生有搭建一个体系要简单太多。而要解决的问题一般就从“无从下手”变成“知道了要做什么但是该怎么做”，当然不是否定很优秀的人的贡献和努力，但是毕竟人还是来自于正态分布，绝大部分人是平庸的，outlier是少数。但是就是因为有个人努力的差别，外部环境的差别，时间分配的差别，等等差别才让平庸之间也能清晰的分层。而这种分层则会导致一个很简单的结论“他们看起来很厉害”。\n既然是随笔其实逻辑不用写太清晰吧（笑），毕竟要写清晰好多东西都得重新改了，根据逻辑线条重新安排写作，就随缘吧。\n这里说一下割裂感，对工程的恐惧。从0开始搭建一个项目，虽然一开始每一步都是重复前人工作，但是整个工作量会令人绝望的大。而导师的作用是拉你到山头，从山头开始建立体系，这样能避免曲曲绕绕被工作量压倒从而没法积累。但是目前的工程又很一言难尽，虽然师兄的代码确实帮了不少忙，但是（笑）。 离题太远了，还是说事情吧。和2012可信理论技术工程实验室的人聊了聊，他们在做知识图谱，其中用到了causal inference的东西。说个很囧的事情就是我一直以为是casual（casualty那个casual），但是实际上是(cause的causal)，还连续被纠正两次orz，有点惨（笑）。然后找上我的原因是我做bayes，然后这种人很难找，听他们聊天好像来英国一共就没碰到几个。其实我离causal inference还挺远的，大概就是做自然语言处理和做计算机视觉的差别吧，虽然基础都是深度学习那些东西。不过既然用图模型，precision阵分解也要用可能也没那么远？那得看了以后才知道。好像又偏题了，反正结论就是陷入数学系PhD常见沉思，我在干什么，我干的有啥用，我为什么那么菜。虽然之前刷论文和书的时候这种菜的感觉减弱了一点，但是总感觉能一句话总结我学了啥的时候还是觉得自己好菜。哦对，自己辛苦努力研究入门了的Rcpp啊，会一点的python啊什么的跨学科知识好像根本没人关心（笑），只关心最前沿的技术和能用的理论，虽然早就知道但是还是有点那啥，虽然不能说废了很多功夫但是其实也不算少。还是多少会有点失落。虽然还是想继续学，但是不学深根本没用，突然想到，就有点像异化。虽然肯定比喻不恰当，但是谁在乎呢。 说了一堆废话，说好的读后感呢。嗯其实还是有关系，就在博士围城理论里面（笑）。\n同理最近对正反馈理论非常痴迷，大概是因为我对我自我认识的问题，对 “我想要，我能做，我应该做”这三个命题的认识又十分割裂，所以之前并不是很重视细节上的正反馈。而且还出现了喜欢+想做什么，但是偏偏又不能做这种烦心的状态。因为我是平庸的人，坚持什么的做不到啦。但是呢，因为画肌肉结构的时发现把二三头肌分开画就很像手臂。因为这个就很想练二头，然后去gym就是一件更开心的事而不是煎熬了，那就去练吧，这样又多练一点点了。坚持什么的做不到，可能过两天就失去兴趣了，缺乏延迟享受的人就这样，我们平庸的人就这样（笑），也不想做到，自己做的开心就很好，希望能这样。\n嗯其实最后一点也是对应的！把小事通通发上来哈哈。 其实感觉更深刻的是后面那段，注意力分散和拖延症那块。其实我也有很重的这方面毛病，所以感触很深，但是现在突然不想写了，想把那篇文相关链接看完，所以就暂时先这样吧。\n要不然先挖个坑？免得忘了\n 注意力分散，拖延症 毅力 爱好，打羽毛球 有质量的输出  不过这个博客有没有人读呢，感觉没人（笑），没有反馈咋办，剪一下丢到微博上好了。\n啊，后面第二个关键词，甜头里面，我现在就是第一阶段吧，表现欲+碎碎念。迷惑行为哈哈。\n","date":1573603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573603200,"objectID":"f9e10ac8262dcba7aefbf8dea6d2ca7d","permalink":"/post/2019-11-13-/","publishdate":"2019-11-13T00:00:00Z","relpermalink":"/post/2019-11-13-/","section":"post","summary":"作为谢大的迷弟，如何想静静 读后颇有感触，随便写点。也算是另一种程度上的静静。虽然高质量产出很缺乏也很难，但是低质量产出或多或少还是可以做。\n先说第一部分，昨天参加了华为在曼大的宣讲会，听各方面的研究和各个部门代表讲话。感觉嗯大家都很充实的改变世界，除了我自己在混日子（笑）。这种割裂感来源很多，其中一点大概是做项目的感觉把。我曾经也觉得那些天才好厉害天经地纬啥的智商高果然不一样。但是慢慢的就觉得积累，环境，等等的都会有很大的影响作用。人脑解决问题的方式应该还是简化简化积累简化。也就是说处理的每一个小问题都应该是简单的，只不过这个简单对积累不同的人不一样。能把微积分常见的定理+trick倒背如流做吉米多维奇时的难度肯定就比只是去上课听了一点还记不住的人简单。但是处理的时候还是以处理简单问题为主。而同样类比到博士，成熟的试验手段，代码积累，以及人的问题都很重要。因为很多复杂的问题都是很多人积累之下，而熟悉自己课题组的积累比从论文中无众生有搭建一个体系要简单太多。而要解决的问题一般就从“无从下手”变成“知道了要做什么但是该怎么做”，当然不是否定很优秀的人的贡献和努力，但是毕竟人还是来自于正态分布，绝大部分人是平庸的，outlier是少数。但是就是因为有个人努力的差别，外部环境的差别，时间分配的差别，等等差别才让平庸之间也能清晰的分层。而这种分层则会导致一个很简单的结论“他们看起来很厉害”。\n既然是随笔其实逻辑不用写太清晰吧（笑），毕竟要写清晰好多东西都得重新改了，根据逻辑线条重新安排写作，就随缘吧。\n这里说一下割裂感，对工程的恐惧。从0开始搭建一个项目，虽然一开始每一步都是重复前人工作，但是整个工作量会令人绝望的大。而导师的作用是拉你到山头，从山头开始建立体系，这样能避免曲曲绕绕被工作量压倒从而没法积累。但是目前的工程又很一言难尽，虽然师兄的代码确实帮了不少忙，但是（笑）。 离题太远了，还是说事情吧。和2012可信理论技术工程实验室的人聊了聊，他们在做知识图谱，其中用到了causal inference的东西。说个很囧的事情就是我一直以为是casual（casualty那个casual），但是实际上是(cause的causal)，还连续被纠正两次orz，有点惨（笑）。然后找上我的原因是我做bayes，然后这种人很难找，听他们聊天好像来英国一共就没碰到几个。其实我离causal inference还挺远的，大概就是做自然语言处理和做计算机视觉的差别吧，虽然基础都是深度学习那些东西。不过既然用图模型，precision阵分解也要用可能也没那么远？那得看了以后才知道。好像又偏题了，反正结论就是陷入数学系PhD常见沉思，我在干什么，我干的有啥用，我为什么那么菜。虽然之前刷论文和书的时候这种菜的感觉减弱了一点，但是总感觉能一句话总结我学了啥的时候还是觉得自己好菜。哦对，自己辛苦努力研究入门了的Rcpp啊，会一点的python啊什么的跨学科知识好像根本没人关心（笑），只关心最前沿的技术和能用的理论，虽然早就知道但是还是有点那啥，虽然不能说废了很多功夫但是其实也不算少。还是多少会有点失落。虽然还是想继续学，但是不学深根本没用，突然想到，就有点像异化。虽然肯定比喻不恰当，但是谁在乎呢。 说了一堆废话，说好的读后感呢。嗯其实还是有关系，就在博士围城理论里面（笑）。\n同理最近对正反馈理论非常痴迷，大概是因为我对我自我认识的问题，对 “我想要，我能做，我应该做”这三个命题的认识又十分割裂，所以之前并不是很重视细节上的正反馈。而且还出现了喜欢+想做什么，但是偏偏又不能做这种烦心的状态。因为我是平庸的人，坚持什么的做不到啦。但是呢，因为画肌肉结构的时发现把二三头肌分开画就很像手臂。因为这个就很想练二头，然后去gym就是一件更开心的事而不是煎熬了，那就去练吧，这样又多练一点点了。坚持什么的做不到，可能过两天就失去兴趣了，缺乏延迟享受的人就这样，我们平庸的人就这样（笑），也不想做到，自己做的开心就很好，希望能这样。\n嗯其实最后一点也是对应的！把小事通通发上来哈哈。 其实感觉更深刻的是后面那段，注意力分散和拖延症那块。其实我也有很重的这方面毛病，所以感触很深，但是现在突然不想写了，想把那篇文相关链接看完，所以就暂时先这样吧。\n要不然先挖个坑？免得忘了\n 注意力分散，拖延症 毅力 爱好，打羽毛球 有质量的输出  不过这个博客有没有人读呢，感觉没人（笑），没有反馈咋办，剪一下丢到微博上好了。\n啊，后面第二个关键词，甜头里面，我现在就是第一阶段吧，表现欲+碎碎念。迷惑行为哈哈。","tags":[],"title":"果然还是想静静","type":"post"},{"authors":null,"categories":["Rcpp","Package"],"content":"  首先是在devtools::load_all(“.”)的时候的一个坑，因为mac系统默认的clang不支持-fopenmp, 为了支持得下其他的编译器，比如llvm的clang,然后对R的设定进行修改，对电脑默认文件夹（大概是R的环境变量？）指定编译器：\nCC=/usr/local/opt/llvm/bin/clang CXX=/usr/local/opt/llvm/bin/clang++ CXX11=/usr/local/opt/llvm/bin/clang++ 来自于淼大人的博客 注意这是R的环境变量里面的Makevars，也就是.R/Makevars.大概意思是你的电脑的根目录的.R下面创建一个文件，名字是Makevars,然后再用devtools::load_all(\".\")就不会报错了。\n然后在写BayesJMCM的Rcpp版本的时候遇到的第二个坑是Rcpp::NumericVector和Rcpp::NumericMatrix到arma::vec和arma::mat之间的转换。按Dirk Eddelbuettel大人的写法是\nRcpp::NumericVector yr(ys); Rcpp::NumericMatrix Xr(XS); int n=Xr.nrow(),k=Xr,ncol(); arma::mat X(Xr.begin(),n,k,false); arma::colvec y(yr.begin(),yr.size(),false); 这个是属于armadillo语法手册中“advanced constructor”的用法，However, if copy_aux_mem is set to false, the vector will instead directly use the auxiliary memory (ie. no copying); this is faster, but can be dangerous unless you know what you are doing! ，这个false就是初始化是否copy自该内存空间，如果是false的话就不copy，直接用的是when strict is set to false, the vector will use the auxiliary memory until a size change，也就不分配额外的内存空间，达成重用原始内容，因为大概毕竟底层的结构都是std::vector。\n但是由于我代码是一部分一部分重写的，所以每个函数都是Rcpp::List的输入和输出，所以充满了大量的不必要的显式转换(因为编译器报错)，和隐式转换（编译器没报错，比如返回Rcpp::list使用List::create的时候直接=一个arma::vec和arma::mat）。\n这块应该统一一下，比如只用Rcpp::NumericVector和NumericMatrix在一开始从R接受数据的时候，和返回的时候（隐式）。\n这时候如果放弃使用Rcpp::List做黏胶把要返回的东西（例如两个矩阵），黏在一起的话，得考虑新的东西，比如用std::tuple和ties可能是一个解决方案,但是不是很确定，不过既然armadillo是从这继承的应该问题不大。\n第三个坑是，还是在devtools::load_all(“.”)里面，提示\nError in getDLLRegisteredRoutines.DLLInfo(dll, addNames = FALSE) : must specify DLL via a “DLLInfo” object. See getLoadedDLLs() 这个应该怎么设置？搜索一下没找到太具体的做法，改天继续试，索性一禁了之,把namespace文件里面的useDynLib(packageName, .registration=TRUE),变成useDynLib(packageName)。 但是如果按这篇文章的说法，好像不注册就不能用，不知道下面这个问题和这个有没有关系\nError in .Call(\u0026quot;_BayesJMCM_beta_result_cpp\u0026quot;, PACKAGE = \u0026quot;BayesJMCM\u0026quot;, data, : \u0026quot;_BayesJMCM_beta_result_cpp\u0026quot; not available for .Call() for package \u0026quot;BayesJMCM\u0026quot; 但是暂时不知道咋解决，强行SourceCpp用了还行，从40s左右变成7s左右。\n","date":1564531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564531200,"objectID":"4332bd529091217a1df1e40f31d7c809","permalink":"/post/rcpp/","publishdate":"2019-07-31T00:00:00Z","relpermalink":"/post/rcpp/","section":"post","summary":"首先是在devtools::load_all(“.”)的时候的一个坑，因为mac系统默认的clang不支持-fopenmp, 为了支持得下其他的编译器，比如llvm的clang,然后对R的设定进行修改，对电脑默认文件夹（大概是R的环境变量？）指定编译器：\nCC=/usr/local/opt/llvm/bin/clang CXX=/usr/local/opt/llvm/bin/clang++ CXX11=/usr/local/opt/llvm/bin/clang++ 来自于淼大人的博客 注意这是R的环境变量里面的Makevars，也就是.R/Makevars.大概意思是你的电脑的根目录的.R下面创建一个文件，名字是Makevars,然后再用devtools::load_all(\".\")就不会报错了。\n然后在写BayesJMCM的Rcpp版本的时候遇到的第二个坑是Rcpp::NumericVector和Rcpp::NumericMatrix到arma::vec和arma::mat之间的转换。按Dirk Eddelbuettel大人的写法是\nRcpp::NumericVector yr(ys); Rcpp::NumericMatrix Xr(XS); int n=Xr.nrow(),k=Xr,ncol(); arma::mat X(Xr.begin(),n,k,false); arma::colvec y(yr.begin(),yr.size(),false); 这个是属于armadillo语法手册中“advanced constructor”的用法，However, if copy_aux_mem is set to false, the vector will instead directly use the auxiliary memory (ie. no copying); this is faster, but can be dangerous unless you know what you are doing! ，这个false就是初始化是否copy自该内存空间，如果是false的话就不copy，直接用的是when strict is set to false, the vector will use the auxiliary memory until a size change，也就不分配额外的内存空间，达成重用原始内容，因为大概毕竟底层的结构都是std::vector。","tags":["Rcpp"],"title":"Rcpp使用记录","type":"post"},{"authors":null,"categories":["随笔"],"content":"随便选了这个模板来从头构造博客。 一开始无法render \\$$下的latex代码，不知道为啥，一检查发现连网页中都没有mathjax的js代码。遂去tutorial里面关于latex math formula 那一节 去找，发现没有说，只是说怎么在markdown里面写。最后是在issue里面找到别人提过一嘴，要在在config里面加math=true。。。真坑 加完了还不算完事，公式有一多半显示不了，一看是符号_有问题，改成 _ 以后还有begin{aligned}有问题，omg，因为这是hugo版本的latex render，和pandoc以及其他的亚语法有区别，感觉碰到了大坑orz。所以估计，还得换或者再深入了解一下怎么办再说吧。。。\n第二个坑是在用Rstudio的addin，new post里面提示 \u0026gt; Error: Unable to find theme Directory: /Users/wglaive/Documents/GitHub/Blog_restart/themes/academic\n一看文件夹，template的名字叫hugo-academic\u0026hellip;.orz，直接暴力尝试复制hugo-academic并且重命名为academic，然后就在写这篇。。。希望能成功搞定吧。\n","date":1553385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385600,"objectID":"ac56b6e501089d2a34675665ebaf5de3","permalink":"/post/hugo-academic-math-and-etc/","publishdate":"2019-03-24T00:00:00Z","relpermalink":"/post/hugo-academic-math-and-etc/","section":"post","summary":"随便选了这个模板来从头构造博客。 一开始无法render \\$$下的latex代码，不知道为啥，一检查发现连网页中都没有mathjax的js代码。遂去tutorial里面关于latex math formula 那一节 去找，发现没有说，只是说怎么在markdown里面写。最后是在issue里面找到别人提过一嘴，要在在config里面加math=true。。。真坑 加完了还不算完事，公式有一多半显示不了，一看是符号_有问题，改成 _ 以后还有begin{aligned}有问题，omg，因为这是hugo版本的latex render，和pandoc以及其他的亚语法有区别，感觉碰到了大坑orz。所以估计，还得换或者再深入了解一下怎么办再说吧。。。\n第二个坑是在用Rstudio的addin，new post里面提示 \u0026gt; Error: Unable to find theme Directory: /Users/wglaive/Documents/GitHub/Blog_restart/themes/academic\n一看文件夹，template的名字叫hugo-academic\u0026hellip;.orz，直接暴力尝试复制hugo-academic并且重命名为academic，然后就在写这篇。。。希望能成功搞定吧。","tags":[],"title":"好多坑，Hugo,academic, math and etc...","type":"post"},{"authors":null,"categories":null,"content":" Lack of suitable mutivariate joint distributions for discrete variables that incorporate with the correlation.\nMaximizing the full likelihooe function constructed from copula representation can be computational infeasible\u0026ndash;\u0026gt; Composite pairwise likelihood approach.\nCopula usage: Gaussian copula:Formally:\nA set of random variables $\\boldsymbol{U}=\\left(U _ {1}, \\dots, U _ {d}\\right)^{\\mathrm{T}}$ follows a Gaussian copula model if their joint distribution is specified by $$ F\\left(u _ {1}, \\ldots, u _ {d}\\right)=P\\left(U _ {1} \\leq u _ {1}, \\ldots, U _ {d} \\leq u _ {d}\\right)=\\Phi _ {d}\\left(v _ {1}, \\ldots, v _ {d} ; \\mathbf{R}\\right) $$ $\\Phi _ d$ is the porbability distribution function of the d-dimensional standardised normal distribution with zero mean, $\\mathbf R$ is the correlation matrix.\n$v _ i=\\Phi^{-1} (w _ i)$ Where $w _ i=P(U _ i\\leq u _ i)$ is the marginal distribution of $U _ i\\leq i\\leq d$.\nOther copulas, like t-copula can also be applied.\nNotation: standard Longitudinal data format: $\\mathbf{y} _ {i}=\\left(y _ {i 1}, \\ldots, y _ {i m _ {i}}\\right)^{\\mathrm{T}}$ , $m _ i$ longitudinal measurements fr i-th subject, discrete response $y _ {ij}$ is observed at time $t _ {ij}$ .Consider $y _ {i j} \\in{0,1,2, \\ldots}$ . and $\\mathbf{t} _ {i}=\\left(t _ {i 1}, \\ldots, t _ {i m _ {i}}\\right)^{\\mathrm{T}}$\n${ y _ {ij},x _ {ij},t _ {ij} }(i=1,\u0026hellip;,n;j=1,\u0026hellip;,m _ i).$ For categorical responses, we assume that $y _ {ij}$ follows exponential family distribution (so that GLMs can be used modelling marginally discrete responses.) $$ f(y)=c(y ; \\varphi) \\exp [{y \\theta-\\psi(\\theta)} / \\varphi] $$ Since $\\psi\u0026rsquo;(\\theta)=E(Y):=\\mu$.\ncanonical link function: $$ (\\psi\u0026rsquo;)^{-1} (\\mu)=g(\\mu) $$ Then the GLM marginally for each $y _ {ij}$ as $$ g(E(y _ {ij}))=g(\\mu _ {ij})=x^T _ {ij}\\beta $$ $\\operatorname{var}(y)=\\varphi \\psi^{\\prime \\prime}(\\theta)\u000b$\nJoint distribution of $y _ i$ following the Gaussian copula representation $$ F _ {m _ i}(y _ i)=P(Y _ {i1}\\leq y _ {i1},\u0026hellip;,Y _ {im _ i}\\leq y _ {im _ i})=\\Phi _ {m _ i}(z _ {i1},\u0026hellip;,z _ {im _ i};\\mathbf R _ i) $$ where $z _ {ij}=\\Phi^{-1} _ 1 {F(y _ {ij})}$\nThis is, $F(y _ {ij})$ is the probability of $Y _ {i _ 1} \\leq y _ {i1}$ , it\u0026rsquo;s in the probability space [0,1], $\\Phi _ 1^{-1} {F(y _ {ij})}$ make the value from probability space back to sample space. Then this is in continuous sample space, then use multivariate Normal distribution as the joint distribution.\nIn a special case when the responses are binary, the correlation between two observations is a monotone function of the corresponding element in $R _ i$ .\nThe HPC insed in decomposite correlation coefficients $R _ i$ .\n\u0026hellip;\nThe description of HPC\n\u0026hellip;\nmodelling angles by $$ \\omega _ {i j k}=\\pi / 2-\\operatorname{atan}\\left(\\mathbf{w} _ {i j k}^{\\mathrm{T}} \\boldsymbol{\\gamma}\\right) $$ $\\mathbf{w} _ {ijk}\\in \\mathbb R^{q}$ is a covariate and $\\mathbb \\gamma$ is a $q\\times 1$ unknown parameters.\nLet $\\boldsymbol{\\theta}=\\left(\\boldsymbol{\\beta}^{\\mathrm{T}}, \\boldsymbol{\\gamma}^{\\mathrm{T}}, \\varphi^{\\mathrm{T}}\\right)$. Then follow the framework upon, we can construct likelihood and estimate $\\mathbb \\theta$ by MLE.\nGaussian copula has continuous support on $\\mathbb R^d$ while discrete response variable are defined only on discrete grid points.\nFull likelihood:\n$$ \\begin{aligned} L(\\boldsymbol{\\theta}) \u0026amp;=\\prod _ {i=1}^{n} P\\left(Y _ {i 1}=y _ {i 1}, \\ldots, Y _ {i m _ {i}}=y _ {i m _ {i}}\\right) \\ \u0026amp;=\\prod _ {i=1}^{n} P\\left(y _ {i 1}-1\u0026lt;Y _ {i 1} \\leq y _ {i 1} \\leq y _ {i 1}, \\ldots, y _ {i m _ {i}}-1\u0026lt;Y _ {i m _ {i}} \\leq y _ {i m _ {i}}\\right) \\ \u0026amp;=\\prod _ {i=1}^{n} \\int \\cdots \\int _ {\\mathbf{Z} _ {i}^{-}\u0026lt;\\mathbf{u} \\leq \\mathbf{Z} _ {i}} \\phi _ {m _ {i}}\\left(\\mathbf{u} ; \\mathbf{R} _ {i}\\right) d \\mathbf{u} \\end{aligned} $$ 积分形式应该来源于Gaussian copula的定义。\n Laplace 展开对u，精度可能不够，特别是binary\n包括pairwise likelihood，需要满足一定条件才能逼近的好\n This is hard to deal with, then the author use the composite likelihood by using pairwise likelihood.\nConstruct all pairwise likelihoods via bivariate copula as $$ p L(\\boldsymbol{\\theta})=\\prod _ {i=1}^{n} \\prod _ {1 \\leq j\u0026lt;k \\leq m _ {i}} \\int _ {z _ {i j}^{-}}^{z _ {i j}} \\int _ {z _ {i k}^{-}}^{z _ {i k}} \\phi _ {2}\\left(\\mathbf{u} ; \\rho _ {i j k}\\right) d \\mathbf{u} $$ That is, using two-integral instead of $m _ i$ integrals.\n$\\rho _ {ijk}$ is specified by the HPC.\nlog pairwise likelihood function is $$ p l(\\boldsymbol{\\theta})=\\sum _ {i=1}^{n} \\sum _ {1 \\leq j\u0026lt;k \\leq m _ {i}} \\log \\int _ {z _ {i j}^{-}}^{z _ {i j}} \\int _ {z _ {i k}^{-}}^{z _ {i k}} \\phi _ {2}\\left(\\mathbf{u} ; \\rho _ {i j k}\\right) d \\mathbf{u} :=\\sum _ {i=1}^{n} \\sum _ {1 \\leq j\u0026lt;k \\leq m _ {i}} l _ {i j k}(\\theta) $$ and the score function is $$ \\mathbf{S} _ {n}(\\boldsymbol{\\theta})=\\frac{\\partial p l}{\\partial \\boldsymbol{\\theta}}=\\sum _ {i=1}^{n} \\sum _ {1 \\leq j\u0026lt;k \\leq m _ {i}} \\frac{\\partial l _ {i j k}}{\\partial \\boldsymbol{\\theta}} :=\\sum _ {i=1}^{n} \\mathbf{S} _ {n i}(\\boldsymbol{\\theta}) $$ Employ modified Fisher scoring algorithm to maximize the pairwise likelihood function.\nSong, et. al. 2009\nJoint Regression Analysis of Correlated Data Using Gaussian Copulas Estimating equations (EE)-based approach. Join marginal models for correlated outcomes. Shortcomings associated with the EE method due to lack of fuly parametric model: 1. loss of estimation efficiency 2. the lack of procedures for model assessment and selection, 3. Difficulty of incorporating vector outcomes of mixed types.\nBurn injury data: $y _ 1=log(\\textit{burn area}+1)$ $y _ 2$ 1 for death from burn injury and 0 for survival.\nNormally people do two seperate regression model but the problem is $y _ 1$ and $y _ 2$ are not independent, it\u0026rsquo;s joint. How to consider the correlation in these two marginal models?\nDevelop a unified and flexible likelihood framework to join various marginal models.\nUse GLMs as marginal models. To join marginal GLMs, invoke Gaussian copulas as the link model, and the resulting joint regression model is refrred to as the vector GLM(VGLM).\nVGLM for correlated discrete outcomes and correlated mixed outcomes. Comparisons to the moment-based EE approach.\nJointly analyze vector data by the GLM approach multidimensional GLMs, or VGLMs, specify the conditional distribution of a vector response y given x as follows : $$ f(\\mathbf{y} | \\mathbf{x} ; \\boldsymbol{\\beta}, \\boldsymbol{\\varphi}, \\Gamma)=\\delta\\left(\\mathbf{y}, \\eta _ {1}, \\ldots, \\eta _ {m} ; \\boldsymbol{\\varphi}, \\Gamma\\right) $$ Parametric link model $\\delta(\\cdot ;\\varphi,\\Gamma)\u000b$ is parameterized by the vector of dispersion parameters $\\boldsymbol \\varphi=(\\varphi _ 1,\u0026hellip;,\\varphi _ m)^T\u000b$\nThe $\\Gamma\u000b$ characterizes the association among the components of $\\mathbf y\u000b$.\nThis article consider a new class of parametric link models $\\delta(\\cdot)$ via multivariate distributions generated by Gaussian copulas.\nMultivariate ED Family distribution\nCumulative distribution function of $ED(\\mu _ j,\\varphi _ j)$ denoted by $G _ j(y _ j;\\mu _ j,\\varphi _ j)$ $$ F(\\mathbf{y} ; \\boldsymbol{\\mu}, \\varphi, \\Gamma)=C\\left{G _ {1}\\left(y _ {1} ; \\mu _ {1}, \\varphi _ {1}\\right), \\ldots, G _ {m}\\left(y _ {m} ; \\mu _ {m}, \\varphi _ {m}\\right) | \\Gamma\\right} $$ $\\boldsymbol \\mu=(\\mu _ 1,\\cdots,\\mu _ m)^T\u000b$ is the vector of m means,$\\boldsymbol\\varphi=(\\varphi _ 1,\u0026hellip;,\\varphi _ m)^T\u000b$ is dispersion parameters, $C(\\cdot)\u000b$ is m-variate Gaussian copula with $$ \\begin{aligned} C(\\mathbf{u} | \\Gamma) \u0026amp;=\\Phi _ {m}\\left{\\Phi^{-1}\\left(u _ {1}\\right), \\ldots, \\Phi^{-1}\\left(u _ {m}\\right) | \\Gamma\\right} \\ \\mathbf{u} \u0026amp;=\\left(u _ {1}, \\ldots, u _ {m}\\right)^{T} \\in(0,1)^{m} \\end{aligned} $$ $\\Gamma$ is Pearson correlation matrix. For non-Gaussian margins, $\\Gamma$ becomes a pairwise non-linear association, van der Waerden coefficient, which is defined by $$ \\gamma _ {i j}=\\operatorname{corr}\\left[\\Phi^{-1}\\left{G _ {i}\\left(y _ {i}\\right)\\right}, \\Phi^{-1}\\left{G _ {j}\\left(y _ {j}\\right)\\right}\\right] $$ For continuous marginal CDF, $\\gamma _ {ij}$ represents the linear correlation of two normal scores $\\Phi^{-1}\\left{G _ {t}\\left(y _ {t}\\right)\\right}, t=i, j$ . For discrete cases, the equation still holds but interpretation would be different. For binary case with same joint probability mass function, then the $\\gamma _ {ij}\u000b$ can be interpreted as the tetrachoric correlation.\nThe density functions of MEDs with different marginal distribution, in continuous case, the MED can be expressed by $$ f(\\mathbf{y} ; \\boldsymbol{\\mu}, \\varphi, \\Gamma)=c\\left{G _ {1}\\left(y _ {1}\\right), \\ldots, G _ {m}\\left(y _ {m}\\right) | \\Gamma\\right} \\prod _ {i=1}^{m} g _ {i}\\left(y _ {i} ; \\mu _ {i}, \\varphi _ {i}\\right) $$ where $c(\\cdot)$ is density of the copula $C( \\cdot)$ $$ c(\\mathbf{u} | \\Gamma)=|\\Gamma|^{-1 / 2} \\exp \\left{\\frac{1}{2} \\mathbf{q}^{T}\\left(I _ {m}-\\Gamma^{-1}\\right) \\mathbf{q}\\right} $$ When all margins are discrete, the joint probability function of a discrete MED distribution with the form: $$ \\begin{aligned} f(\\mathbf{y}) \u0026amp;=\\mathrm{P}\\left(Y _ {1}=y _ {1}, \\ldots, Y _ {m}=y _ {m}\\right) \\ \u0026amp;=\\sum _ {j _ {1}=1}^{2} \\cdots \\sum _ {j _ {m}=1}^{2}(-1)^{j _ {1}+\\cdots+j _ {m}} C\\left(u _ {1, j _ {1}}, \\ldots, u _ {m, j _ {m}} | \\Gamma\\right) \\end{aligned} $$ Another case is the mixed outcome.\n Simultaneous Maximum Likelihood Inference  Responses $\\left(\\mathbf{y} _ {1}, \\ldots, \\mathbf{y} _ {n}\\right)$ with covariates $\\left(X _ {1}, \\ldots, X _ {n}\\right)$ follows m-variate MED distribution $$ \\mathbf{y} _ {i}\\left|X _ {i}=\\left(\\mathbf{x} _ {i 1}, \\ldots, \\mathbf{x} _ {i m}\\right) \\sim \\operatorname{MED} _ {m}\\left(\\boldsymbol{\\mu} _ {i}, \\boldsymbol{\\varphi} _ {i}, \\Gamma\\right), \\quad i=1, \\ldots, n\\right. $$ Responses $\\bold y _ i$ has mean $\\bold \\mu _ i=\\left(\\mu _ {i 1}\\left(\\mathbf{x} _ {i 1}\\right), \\ldots, \\mu _ {i m}\\left(\\mathbf{x} _ {i m}\\right)\\right)^{T}$ and dispersion $\\varphi _ {i}=\\left(\\varphi _ {i 1}, \\ldots, \\varphi _ {i m}\\right)^{T}$ .\nMean $\\mu _ i$ follows marginal GLM $h _ j(\\mu _ {ij})=\\eta _ j(x _ {ij}) $ with $\\eta _ {i j}=\\mathbf{x} _ {i j}^{T} \\boldsymbol{\\beta} _ {j}$\nLet $\\theta=(\\beta,\\varphi,\\Gamma)$ . For normal longitudinal or clustered data analysis, VGLM with $\\Gamma$ has some specific form like AR(1), that is $\\Gamma(\\alpha)$\nLoglikelihood： $\\ell(\\boldsymbol{\\theta} ; Y, X)=\\sum _ {i=1}^{n} \\ell _ {i}\\left(\\boldsymbol{\\theta} ; \\mathbf{y} _ {i}, X _ {i}\\right)$，then MLE $\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\ell(\\boldsymbol{\\theta} ; Y, X)$ .\nSearch for result we use Gauss-Newton type algorithm.\nThe observed fisher Information using the sandwich form: $\\hat{\\mathcal{I}}=\\mathbf{A} _ {n}^{-1}(\\hat{\\boldsymbol{\\theta}}) \\mathbf{B} _ {n}(\\hat{\\boldsymbol{\\theta}}) \\mathbf{A} _ {n}^{-1}(\\hat{\\boldsymbol{\\theta}})$ where $\\mathbf{A} _ {n}(\\theta)$ is the numerical Hessian matrix and $B _ n(\\theta)=\\frac{1}{n} \\sum _ {i=1}^{n} \\dot{\\ell} _ {i}\\left(\\boldsymbol{\\theta} ; \\mathbf{y} _ {i}, X _ {i}\\right) \\dot{\\ell} _ {i}\\left(\\boldsymbol{\\theta} ; \\mathbf{y} _ {i}, X _ {i}\\right)^{T}$ . The iteration updates of parameter $\\theta$ is by $$ \\theta^{k+1}=\\theta^{k}+\\epsilon\\left{\\mathbf{B} _ {n}\\left(\\theta^{k}\\right)\\right}^{-1} \\dot{\\ell}\\left(\\theta^{k}\\right) $$ $\\epsilon$ is step-halving term. Among Newton-Raphson, downhill simplex, quasi-Newton with numerical derivatives, the Gauss-Newton appears to privde the best trade-off.\n VGLMs for Trivariate Discrete Data  Moment-based EE method .etc\nTrivariate VGLMs\nprobability mass function: $$ \\begin{aligned} f\\left(\\mathbf{y} _ {i} ; \\boldsymbol{\\theta}\\right) \u0026amp;=P\\left(Y _ {i 1}=y _ {i 1}, Y _ {i 2}=y _ {i 2}, Y _ {i 3}=y _ {i 3}\\right) \\ \u0026amp;=\\sum _ {j _ {1}=1}^{2} \\sum _ {j _ {2}=1}^{2} \\sum _ {j _ {3}=1}^{2}(-1)^{j _ {1}+j _ {2}+j _ {3}} C\\left(u _ {i, 1, j _ {1}}, u _ {i, 2, j _ {2}}, u _ {i, 3, j _ {3}} | \\alpha\\right) \\end{aligned} $$ where $C\\left(u _ {i, 1, j _ {1}}, u _ {i, 2, j _ {2}}, u _ {i, 3, j _ {3}} | \\alpha\\right)=\\Phi _ {3}\\left{\\Phi^{-1}\\left(u _ {i, 1, j _ {1}}\\right), \\Phi^{-1}\\left(u _ {i, 2, j _ {2}}\\right)\\right. ,\\Phi^{-1}\\left(u _ {i, 3, j _ {3}}\\right) | \\alpha }$with $\\alpha$ is exchangeable correlation coefficient $\\alpha$\n","date":1550966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550966400,"objectID":"a6bdb25365b552de528c273ea5d483b0","permalink":"/post/copula-for-discrete-lda/copula-for-discrete-lda/","publishdate":"2019-02-24T00:00:00Z","relpermalink":"/post/copula-for-discrete-lda/copula-for-discrete-lda/","section":"post","summary":"Lack of suitable mutivariate joint distributions for discrete variables that incorporate with the correlation.\nMaximizing the full likelihooe function constructed from copula representation can be computational infeasible\u0026ndash;\u0026gt; Composite pairwise likelihood approach.\nCopula usage: Gaussian copula:Formally:\nA set of random variables $\\boldsymbol{U}=\\left(U _ {1}, \\dots, U _ {d}\\right)^{\\mathrm{T}}$ follows a Gaussian copula model if their joint distribution is specified by $$ F\\left(u _ {1}, \\ldots, u _ {d}\\right)=P\\left(U _ {1} \\leq u _ {1}, \\ldots, U _ {d} \\leq u _ {d}\\right)=\\Phi _ {d}\\left(v _ {1}, \\ldots, v _ {d} ; \\mathbf{R}\\right) $$ $\\Phi _ d$ is the porbability distribution function of the d-dimensional standardised normal distribution with zero mean, $\\mathbf R$ is the correlation matrix.","tags":null,"title":"Notes on Discrete Longitudinal Data Modeling with a Mean-Correlation Regression Approach","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536447600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536447600,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+01:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441062000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441062000,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00+01:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372633200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372633200,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00+01:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"A mobile visual clothing search system is presented whereby a smart phone user can either choose a social networking image or capture a new photo of a person wearing clothing of interest and search for similar clothing in a large cloud-based ecommerce database. The phone's GPS location is used to re-rank results by retail store location, to inform the user of local stores where similar clothing items can be tried on.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]