<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Academic</title>
    <link>/post/</link>
    <description>Recent content in Posts on Academic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Sep 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用Horseshoe 先验的Bayes回归及代码解析</title>
      <link>/post/horseshoe-bayes/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/horseshoe-bayes/</guid>
      <description>使用Horseshoe 先验的Bayes回归及代码解析 Horseshoe prior是一种稀疏bayes监督学习的方法。通过对模型参数的先验分布中加入稀疏特征，从而得到稀疏的估计。
horseshoe prior属于multivariate scale mixtures of normals的分布族。所以和其他常用的稀疏bayes学习方法，Laplacian prior, (Lasso), Student-t prior，非常类似。
在回归问题下的模型假设： \[ y=X\beta+\epsilon, \epsilon\sim N(0,\sigma^2)\\ \beta_j \sim N(0,\sigma^2 \lambda_j^2 \tau^2)\\ \lambda_j \sim Half-Cauchy(0,1) \] 可以额外加上以下超参数模型 \[ \tau \sim Half-Cauchy (0,1)\\ \sigma^2 \sim 1/\sigma^2 \]
其中\(\lambda_j\) 叫做local shrinkage parameter，局部压缩参数，对于不同的\(\theta_j\)可以有不同的压缩系数，\(\tau\) 叫做global shrinkage parameter
叫horseshoe，马蹄的原因大概是shrinkage weights的图像呈现马蹄形。
Carvalho, Polson, and Scott (2010) 给出了几个horseshoe estimator的相关性质以及和传统其他压缩先验方法的比较。
这篇文章给出了一个用于判断各种压缩先验性质的方法: \[ E\left(\theta_{i} \mid y\right)=\int_{0}^{1}\left(1-\kappa_{i}\right) y_{i} p\left(\kappa_{i} \mid y\right) \mathrm{d} \kappa_{i}=\left\{1-E\left(\kappa_{i} \mid y\right)\right\} y_{i} \] 在这种情况下, \(E(\kappa_i|y)\) 就是压缩率。</description>
    </item>
    
    <item>
      <title>果然还是想静静</title>
      <link>/post/2019-11-13-/</link>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-11-13-/</guid>
      <description>作为谢大的迷弟，如何想静静 读后颇有感触，随便写点。也算是另一种程度上的静静。虽然高质量产出很缺乏也很难，但是低质量产出或多或少还是可以做。
先说第一部分，昨天参加了华为在曼大的宣讲会，听各方面的研究和各个部门代表讲话。感觉嗯大家都很充实的改变世界，除了我自己在混日子（笑）。这种割裂感来源很多，其中一点大概是做项目的感觉把。我曾经也觉得那些天才好厉害天经地纬啥的智商高果然不一样。但是慢慢的就觉得积累，环境，等等的都会有很大的影响作用。人脑解决问题的方式应该还是简化简化积累简化。也就是说处理的每一个小问题都应该是简单的，只不过这个简单对积累不同的人不一样。能把微积分常见的定理+trick倒背如流做吉米多维奇时的难度肯定就比只是去上课听了一点还记不住的人简单。但是处理的时候还是以处理简单问题为主。而同样类比到博士，成熟的试验手段，代码积累，以及人的问题都很重要。因为很多复杂的问题都是很多人积累之下，而熟悉自己课题组的积累比从论文中无众生有搭建一个体系要简单太多。而要解决的问题一般就从“无从下手”变成“知道了要做什么但是该怎么做”，当然不是否定很优秀的人的贡献和努力，但是毕竟人还是来自于正态分布，绝大部分人是平庸的，outlier是少数。但是就是因为有个人努力的差别，外部环境的差别，时间分配的差别，等等差别才让平庸之间也能清晰的分层。而这种分层则会导致一个很简单的结论“他们看起来很厉害”。
既然是随笔其实逻辑不用写太清晰吧（笑），毕竟要写清晰好多东西都得重新改了，根据逻辑线条重新安排写作，就随缘吧。
这里说一下割裂感，对工程的恐惧。从0开始搭建一个项目，虽然一开始每一步都是重复前人工作，但是整个工作量会令人绝望的大。而导师的作用是拉你到山头，从山头开始建立体系，这样能避免曲曲绕绕被工作量压倒从而没法积累。但是目前的工程又很一言难尽，虽然师兄的代码确实帮了不少忙，但是（笑）。 离题太远了，还是说事情吧。和2012可信理论技术工程实验室的人聊了聊，他们在做知识图谱，其中用到了causal inference的东西。说个很囧的事情就是我一直以为是casual（casualty那个casual），但是实际上是(cause的causal)，还连续被纠正两次orz，有点惨（笑）。然后找上我的原因是我做bayes，然后这种人很难找，听他们聊天好像来英国一共就没碰到几个。其实我离causal inference还挺远的，大概就是做自然语言处理和做计算机视觉的差别吧，虽然基础都是深度学习那些东西。不过既然用图模型，precision阵分解也要用可能也没那么远？那得看了以后才知道。好像又偏题了，反正结论就是陷入数学系PhD常见沉思，我在干什么，我干的有啥用，我为什么那么菜。虽然之前刷论文和书的时候这种菜的感觉减弱了一点，但是总感觉能一句话总结我学了啥的时候还是觉得自己好菜。哦对，自己辛苦努力研究入门了的Rcpp啊，会一点的python啊什么的跨学科知识好像根本没人关心（笑），只关心最前沿的技术和能用的理论，虽然早就知道但是还是有点那啥，虽然不能说废了很多功夫但是其实也不算少。还是多少会有点失落。虽然还是想继续学，但是不学深根本没用，突然想到，就有点像异化。虽然肯定比喻不恰当，但是谁在乎呢。 说了一堆废话，说好的读后感呢。嗯其实还是有关系，就在博士围城理论里面（笑）。
同理最近对正反馈理论非常痴迷，大概是因为我对我自我认识的问题，对 “我想要，我能做，我应该做”这三个命题的认识又十分割裂，所以之前并不是很重视细节上的正反馈。而且还出现了喜欢+想做什么，但是偏偏又不能做这种烦心的状态。因为我是平庸的人，坚持什么的做不到啦。但是呢，因为画肌肉结构的时发现把二三头肌分开画就很像手臂。因为这个就很想练二头，然后去gym就是一件更开心的事而不是煎熬了，那就去练吧，这样又多练一点点了。坚持什么的做不到，可能过两天就失去兴趣了，缺乏延迟享受的人就这样，我们平庸的人就这样（笑），也不想做到，自己做的开心就很好，希望能这样。
嗯其实最后一点也是对应的！把小事通通发上来哈哈。 其实感觉更深刻的是后面那段，注意力分散和拖延症那块。其实我也有很重的这方面毛病，所以感触很深，但是现在突然不想写了，想把那篇文相关链接看完，所以就暂时先这样吧。
要不然先挖个坑？免得忘了
 注意力分散，拖延症 毅力 爱好，打羽毛球 有质量的输出  不过这个博客有没有人读呢，感觉没人（笑），没有反馈咋办，剪一下丢到微博上好了。
啊，后面第二个关键词，甜头里面，我现在就是第一阶段吧，表现欲+碎碎念。迷惑行为哈哈。</description>
    </item>
    
    <item>
      <title>Rcpp使用记录</title>
      <link>/post/rcpp/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/rcpp/</guid>
      <description>首先是在devtools::load_all(“.”)的时候的一个坑，因为mac系统默认的clang不支持-fopenmp, 为了支持得下其他的编译器，比如llvm的clang,然后对R的设定进行修改，对电脑默认文件夹（大概是R的环境变量？）指定编译器：
CC=/usr/local/opt/llvm/bin/clang CXX=/usr/local/opt/llvm/bin/clang++ CXX11=/usr/local/opt/llvm/bin/clang++ 来自于淼大人的博客 注意这是R的环境变量里面的Makevars，也就是.R/Makevars.大概意思是你的电脑的根目录的.R下面创建一个文件，名字是Makevars,然后再用devtools::load_all(&#34;.&#34;)就不会报错了。
然后在写BayesJMCM的Rcpp版本的时候遇到的第二个坑是Rcpp::NumericVector和Rcpp::NumericMatrix到arma::vec和arma::mat之间的转换。按Dirk Eddelbuettel大人的写法是
Rcpp::NumericVector yr(ys); Rcpp::NumericMatrix Xr(XS); int n=Xr.nrow(),k=Xr,ncol(); arma::mat X(Xr.begin(),n,k,false); arma::colvec y(yr.begin(),yr.size(),false); 这个是属于armadillo语法手册中“advanced constructor”的用法，However, if copy_aux_mem is set to false, the vector will instead directly use the auxiliary memory (ie. no copying); this is faster, but can be dangerous unless you know what you are doing! ，这个false就是初始化是否copy自该内存空间，如果是false的话就不copy，直接用的是when strict is set to false, the vector will use the auxiliary memory until a size change，也就不分配额外的内存空间，达成重用原始内容，因为大概毕竟底层的结构都是std::vector。</description>
    </item>
    
    <item>
      <title>好多坑，Hugo,academic, math and etc...</title>
      <link>/post/hugo-academic-math-and-etc/</link>
      <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/hugo-academic-math-and-etc/</guid>
      <description>随便选了这个模板来从头构造博客。 一开始无法render \$$下的latex代码，不知道为啥，一检查发现连网页中都没有mathjax的js代码。遂去tutorial里面关于latex math formula 那一节 去找，发现没有说，只是说怎么在markdown里面写。最后是在issue里面找到别人提过一嘴，要在在config里面加math=true。。。真坑 加完了还不算完事，公式有一多半显示不了，一看是符号_有问题，改成 _ 以后还有begin{aligned}有问题，omg，因为这是hugo版本的latex render，和pandoc以及其他的亚语法有区别，感觉碰到了大坑orz。所以估计，还得换或者再深入了解一下怎么办再说吧。。。
第二个坑是在用Rstudio的addin，new post里面提示 &amp;gt; Error: Unable to find theme Directory: /Users/wglaive/Documents/GitHub/Blog_restart/themes/academic
一看文件夹，template的名字叫hugo-academic&amp;hellip;.orz，直接暴力尝试复制hugo-academic并且重命名为academic，然后就在写这篇。。。希望能成功搞定吧。</description>
    </item>
    
    <item>
      <title>Notes on Discrete Longitudinal Data Modeling with a Mean-Correlation Regression Approach</title>
      <link>/post/copula-for-discrete-lda/copula-for-discrete-lda/</link>
      <pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/copula-for-discrete-lda/copula-for-discrete-lda/</guid>
      <description>Lack of suitable mutivariate joint distributions for discrete variables that incorporate with the correlation.
Maximizing the full likelihooe function constructed from copula representation can be computational infeasible&amp;ndash;&amp;gt; Composite pairwise likelihood approach.
Copula usage: Gaussian copula:Formally:
A set of random variables $\boldsymbol{U}=\left(U _ {1}, \dots, U _ {d}\right)^{\mathrm{T}}$ follows a Gaussian copula model if their joint distribution is specified by $$ F\left(u _ {1}, \ldots, u _ {d}\right)=P\left(U _ {1} \leq u _ {1}, \ldots, U _ {d} \leq u _ {d}\right)=\Phi _ {d}\left(v _ {1}, \ldots, v _ {d} ; \mathbf{R}\right) $$ $\Phi _ d$ is the porbability distribution function of the d-dimensional standardised normal distribution with zero mean, $\mathbf R$ is the correlation matrix.</description>
    </item>
    
  </channel>
</rss>